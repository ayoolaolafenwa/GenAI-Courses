{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c88ea4b8",
   "metadata": {},
   "source": [
    "Step-by-Step Guide: Full Code to Build Agentic RAG Using Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9a4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Export OpenAI and TAVILY API Keys \"\"\"\n",
    "%env OPENAI_API_KEY=Your OpenAI API Key\n",
    "%env TAVILY_API_KEY=Your TAVILY API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40d9f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39687fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(pdf_path: str, page1_on_second: bool = True) -> str:\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    pages = loader.load()\n",
    "    parts = []\n",
    "    \n",
    "    for p in pages:\n",
    "        idx0 = p.metadata.get(\"page\", 0)  \n",
    "        label = idx0 if page1_on_second else (idx0 + 1)\n",
    "        parts.append(f\"[PAGE {label}]\\n{p.page_content.strip()}\")\n",
    "\n",
    "    return \"\\n\\n\".join(parts).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "399cfe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from typing import List\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "import re\n",
    "\n",
    "def chunk_data(\n",
    "    text: str,\n",
    "    chunk_size: int = 1500,\n",
    "    chunk_overlap: int = 225\n",
    ") -> List[str]:\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    if \"sentencizer\" not in nlp.pipe_names:\n",
    "        nlp.add_pipe(\"sentencizer\")\n",
    "\n",
    "    splitter = TokenTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        encoding_name=\"cl100k_base\",\n",
    "    )\n",
    "\n",
    "    chunks: List[str] = []\n",
    "\n",
    "    parts = re.split(r\"\\n?\\[PAGE\\s+(\\d+)\\]\\n\", text)\n",
    "    it = iter(parts[1:])\n",
    "    for page_label, page_text in zip(it, it):\n",
    "        sentences = [s.text.strip() for s in nlp(page_text).sents if s.text.strip()]\n",
    "        page_chunks = splitter.split_text(\"\\n\".join(sentences))\n",
    "        for chunk in page_chunks:\n",
    "            chunks.append(f\"[Page {page_label}] {chunk}\")\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf89467b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client.models import VectorParams, Distance\n",
    "from qdrant_client.models import PointStruct\n",
    "from qdrant_client import QdrantClient\n",
    "\n",
    "qdrant_client = QdrantClient(\":memory:\")\n",
    "collection_name = \"vector_store\"\n",
    "qdrant_client.create_collection(\n",
    "    collection_name = collection_name,\n",
    "    vectors_config=VectorParams(\n",
    "        size=1536,\n",
    "        distance=Distance.COSINE,\n",
    "),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "121e924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "def create_upload_embeddings(chunks):\n",
    "    model_name = \"text-embedding-3-small\"\n",
    "    response = openai_client.embeddings.create(input=chunks, model=model_name)\n",
    "\n",
    "    embeddings = [record.embedding for record in response.data]\n",
    "\n",
    "    points = [\n",
    "        PointStruct(\n",
    "            id=idx,\n",
    "            vector=vec,\n",
    "            payload={\"text\": text},\n",
    "        )\n",
    "        for idx, (vec, text) in enumerate(zip(embeddings, chunks))\n",
    "    ]\n",
    "\n",
    "    qdrant_client.upsert(\n",
    "        collection_name=collection_name,\n",
    "        wait=True,\n",
    "        points=points,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ab2a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_pdf(\"Veo-3-Model-Card.pdf\")\n",
    "chunks = chunk_data(text)\n",
    "\n",
    "create_upload_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ddda636",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_document(query: str, top_k: int = 5) -> str:\n",
    "    model_name = \"text-embedding-3-small\"\n",
    "    query_embedding = openai_client.embeddings.create(\n",
    "        input=[query], model=model_name\n",
    "    ).data[0].embedding\n",
    "\n",
    "    results = qdrant_client.query_points(collection_name, query_embedding, limit=top_k, with_payload=True)\n",
    "    retrieved_texts = [output.payload[\"text\"] for output in results.points if output.payload and \"text\" in output.payload]\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts) if retrieved_texts else \"no relevant context found\"\n",
    "\n",
    "    output = f\"\"\"Based on the following context:\n",
    "        <context>\n",
    "        {context}\n",
    "        </context>\n",
    "\n",
    "        Provide a relevant response to:\n",
    "\n",
    "        <query>\n",
    "        {query}\n",
    "        </query>\n",
    "        \"\"\".strip()\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bd6d596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "def web_search(query: str, num_results: int = 10):\n",
    "    try:\n",
    "        result = tavily.search(\n",
    "            query=query,\n",
    "            search_depth=\"basic\",\n",
    "            max_results=num_results,\n",
    "            include_answer=False,       \n",
    "            include_raw_content=False,\n",
    "            include_images=False\n",
    "        )\n",
    "\n",
    "        results = result.get(\"results\", [])\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"results\": results, \n",
    "            \"sources\": [\n",
    "                {\"title\": r.get(\"title\", \"\"), \"url\": r.get(\"url\", \"\")}\n",
    "                for r in results\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"error\": f\"Search error: {e}\",\n",
    "            \"query\": query,\n",
    "            \"results\": [],\n",
    "            \"sources\": [],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62cf8d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_schemas = [\n",
    "   {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"retrieve_document\",\n",
    "        \"description\": \"\"\"\n",
    "        \"Search the internal PDF file containing Veo Model Card.\n",
    "        Use this tool when the user requests information about Veo 3\n",
    "        that only appear in this document and\n",
    "        for every answer you give include page-number citations in the form [page. X]. \n",
    "        \"\"\",\n",
    "        \"strict\": True,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Query to be searched in the PDF corpus.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "    \n",
    "   {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"web_search\",\n",
    "        \"description\": \"\"\"Execute a web search to fetch up to date information. Synthesize a concise, \n",
    "        self-contained answer from the content of the results of the visited pages.\n",
    "        Fetch pages, extract text, and provide the best available result while citing 1-3 sources (title + URL). \"\n",
    "        If sources conflict, surface the uncertainty and prefer the most recent evidence.\n",
    "        \"\"\",\n",
    "        \"strict\": True,\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Query to be searched on the web.\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17b3085a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant:  Reasoning ....\n",
      "Assistant:  Reasoning ....\n",
      "Assistant:  Veo 3 was trained on Google’s Tensor Processing Units (TPUs), including large TPU Pods and distributed training across multiple TPU devices [page. 2].\n",
      "Assistant:  Reasoning ....\n",
      "Assistant:  Reasoning ....\n",
      "Assistant:  London (as of 2025-09-29 12:03 local): Sunny, 15°C (59°F); wind N ~5 km/h; humidity 77% — source: WeatherAPI (https://www.weatherapi.com/).\n",
      "Assistant:  Reasoning ....\n",
      "Assistant:  7/5 = 1.4\n",
      "Exiting chat. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "import json\n",
    "\n",
    "# tracker for the last model’s response id to maintain conversation’s state \n",
    "prev_response_id = None\n",
    "\n",
    "# a list for storing tool’s results from the function call \n",
    "tool_results = []\n",
    "\n",
    "while True:\n",
    "    # if the tool results is empty prompt message \n",
    "    if len(tool_results) == 0:\n",
    "        user_message = input(\"User: \")\n",
    "\n",
    "        \"\"\" commands for exiting chat \"\"\"\n",
    "        if isinstance(user_message, str) and user_message.strip().lower() in {\"exit\", \"q\"}:\n",
    "            print(\"Exiting chat. Goodbye!\")\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        # set the user’s messages to the tool results to be sent to the model \n",
    "        user_message = tool_results.copy()\n",
    "    \n",
    "        # clear the tool results for the next call \n",
    "        tool_results = []\n",
    "\n",
    "    # obtain current’s date to be passed into the model as an instruction to assist in decision making\n",
    "    today_date = datetime.now(timezone.utc).date().isoformat()     \n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model = \"gpt-5-mini\",\n",
    "        input = user_message,\n",
    "        instructions=f\"Current date is {today_date}.\",\n",
    "        tools = tool_schemas,\n",
    "        previous_response_id=prev_response_id,\n",
    "        text = {\"verbosity\": \"low\"},\n",
    "        reasoning={\n",
    "            \"effort\": \"low\",\n",
    "        },\n",
    "        store=True,\n",
    "        )\n",
    "    \n",
    "    prev_response_id = response.id\n",
    "\n",
    "    # Handles model response’s output \n",
    "    for output in response.output:\n",
    "        \n",
    "        if output.type == \"reasoning\":\n",
    "            print(\"Assistant: \",\"Reasoning ....\")\n",
    "\n",
    "            for reasoning_summary in output.summary:\n",
    "                print(\"Assistant: \",reasoning_summary)\n",
    "\n",
    "        elif output.type == \"message\":\n",
    "            for item in output.content:\n",
    "                print(\"Assistant: \",item.text)\n",
    "\n",
    "        # checks if the output type is a function call and append the function call’s results to the tool results list\n",
    "        elif output.type == \"function_call\":\n",
    "            # obtain function name \n",
    "            function_name = globals().get(output.name)\n",
    "            # loads function arguments \n",
    "            args = json.loads(output.arguments)\n",
    "            function_response = function_name(**args)\n",
    "            # append tool results list with the the function call’s id and function’s response \n",
    "            tool_results.append(\n",
    "                {\n",
    "                    \"type\": \"function_call_output\",\n",
    "                    \"call_id\": output.call_id,\n",
    "                    \"output\": json.dumps(function_response)\n",
    "                }\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
